{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import StepLR, ExponentialLR\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from os.path import dirname, abspath\n",
    "import sys, os\n",
    "d = dirname(os.path.abspath(''))\n",
    "sys.path.append(d)\n",
    "from implicitdl import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Weierstrass Function Setup\n",
    "# ---------------------------\n",
    "def weierstrass(x, a=0.5, b=3, N=10):\n",
    "    # x: tensor of shape [batch_size]\n",
    "    # We'll truncate the infinite sum to N terms\n",
    "    # f(x) = sum_{n=0}^N a^n cos(b^n * pi * x)\n",
    "    terms = []\n",
    "    for n in range(N+1):\n",
    "        terms.append((a**n) * torch.cos((b**n) * math.pi * x))\n",
    "    return sum(terms)\n",
    "\n",
    "# ---------------------------\n",
    "# Fourier Feature Mapping\n",
    "# ---------------------------\n",
    "def create_fourier_features(x, num_features=16, scale=10.0):\n",
    "    \"\"\"\n",
    "    Create Fourier features for input tensor x.\n",
    "    x: [batch, 1]\n",
    "    Returns [batch, 2*num_features] (since for each frequency we have sin and cos)\n",
    "    \"\"\"\n",
    "    # frequencies from a geometric progression or linear (for simplicity, linear)\n",
    "    frequencies = torch.linspace(1.0, scale, num_features, device=x.device).unsqueeze(0) # [1, num_features]\n",
    "    x_freq = x * frequencies  # broadcasting: [batch, num_features]\n",
    "    sin_features = torch.sin(2*math.pi*x_freq)\n",
    "    cos_features = torch.cos(2*math.pi*x_freq)\n",
    "    return torch.cat([sin_features, cos_features], dim=-1)  # [batch, 2*num_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data\n",
    "torch.manual_seed(0)\n",
    "N_train = 2000\n",
    "x_train = 4*(torch.rand(N_train)-0.5) + 0.5  # random points in [0,1]\n",
    "y_train = weierstrass(x_train) # generate corresponding outputs\n",
    "x_train = x_train.unsqueeze(-1)  # shape [N_train, 1]\n",
    "y_train = y_train.unsqueeze(-1)  # shape [N_train, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_parameters(model):\n",
    "    \"\"\"Move model parameters to a contiguous tensor, and return that tensor.\"\"\"\n",
    "    n = sum(p.numel() for p in model.parameters())\n",
    "    params = torch.zeros(n)\n",
    "    i = 0\n",
    "    for p in model.parameters():\n",
    "        params_slice = params[i:i + p.numel()]\n",
    "        params_slice.copy_(p.flatten())\n",
    "        p.data = params_slice.view(p.shape)\n",
    "        i += p.numel()\n",
    "    return params\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features=1, hidden_features=128, hidden_layers=4, out_features=1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(in_features, hidden_features))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        \n",
    "        for _ in range(hidden_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_features, hidden_features))\n",
    "            # layers.append(nn.Dropout(0.1))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "        \n",
    "        layers.append(nn.Linear(hidden_features, out_features))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.net:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ---------------------------\n",
    "# Define SIREN Network\n",
    "# ---------------------------\n",
    "# Sine activation\n",
    "class Sine(nn.Module):\n",
    "    def __init__(self, omega=30.0):\n",
    "        super().__init__()\n",
    "        self.omega = omega\n",
    "    def forward(self, x):\n",
    "        return torch.sin(self.omega * x)\n",
    "\n",
    "def siren_init(m, w0=30):\n",
    "    with torch.no_grad():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # SIREN initialization\n",
    "            # For first layer: uniform(-1/w0, 1/w0)\n",
    "            # For subsequent layers: uniform(-sqrt(6)/fan_in/ w0, sqrt(6)/fan_in/ w0)\n",
    "            fan_in = m.weight.size(-1)\n",
    "            if hasattr(m, 'is_first') and m.is_first:\n",
    "                bound = 1.0 / fan_in\n",
    "            else:\n",
    "                bound = math.sqrt(6.0 / fan_in) / w0\n",
    "            m.weight.uniform_(-bound, bound)\n",
    "            if m.bias is not None:\n",
    "                m.bias.uniform_(-bound, bound)\n",
    "\n",
    "# Build a small SIREN MLP\n",
    "class SIREN(nn.Module):\n",
    "    def __init__(self, in_features=1, hidden_features=64, hidden_layers=3, out_features=1, w0=30.0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "\n",
    "        # First layer: linear + sine\n",
    "        first_linear = nn.Linear(in_features, hidden_features)\n",
    "        first_linear.is_first = True\n",
    "        layers.append(first_linear)\n",
    "        layers.append(Sine(omega=w0))\n",
    "\n",
    "        # Hidden layers\n",
    "        for i in range(hidden_layers):\n",
    "            lin = nn.Linear(hidden_features, hidden_features)\n",
    "            layers.append(lin)\n",
    "            # layers.append(nn.Dropout(0.1))\n",
    "            layers.append(Sine(omega=1.0)) # subsequent layers have omega=1\n",
    "        \n",
    "        # Final layer: linear only\n",
    "        final_linear = nn.Linear(hidden_features, out_features)\n",
    "        layers.append(final_linear)\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.initialize_siren(w0)\n",
    "\n",
    "    def initialize_siren(self, w0):\n",
    "        for m in self.net:\n",
    "            siren_init(m, w0=w0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "        \n",
    "# ---------------------------\n",
    "# Model with Fourier Features\n",
    "# ---------------------------\n",
    "class FourierMLP(nn.Module):\n",
    "    def __init__(self, in_features=1, num_fourier_features=16, hidden_features=128, hidden_layers=4, out_features=1):\n",
    "        super().__init__()\n",
    "        self.num_fourier_features = num_fourier_features\n",
    "        \n",
    "        # Input size after Fourier mapping\n",
    "        mapped_dim = 2 * num_fourier_features\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(mapped_dim, hidden_features))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        \n",
    "        for _ in range(hidden_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_features, hidden_features))\n",
    "            # layers.append(nn.Dropout(0.1))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "        \n",
    "        layers.append(nn.Linear(hidden_features, out_features))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.net:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, 1]\n",
    "        # Fourier mapping\n",
    "        ffeat = create_fourier_features(x, num_features=self.num_fourier_features)\n",
    "        return self.net(ffeat)\n",
    "\n",
    "# ---------------------------\n",
    "# Multi-scale Loss Function\n",
    "# ---------------------------\n",
    "def multiscale_loss(y_pred, y_true, scales=[1, 2, 4]):\n",
    "    \"\"\"\n",
    "    Compute a multi-scale MSE loss by downsampling both predictions and targets.\n",
    "    y_pred, y_true: [N, 1]\n",
    "    scales: list of integers representing the downsampling factors.\n",
    "\n",
    "    For each scale s:\n",
    "      - downsample by taking every s-th sample (assuming uniform sampling in x)\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    for s in scales:\n",
    "        y_pred_s = y_pred[::s]\n",
    "        y_true_s = y_true[::s]\n",
    "        loss += torch.mean((y_pred_s - y_true_s)**2)\n",
    "    loss = loss / len(scales)\n",
    "    return loss\n",
    "\n",
    "# ---------------------------\n",
    "# Implicit Models\n",
    "# ---------------------------\n",
    "class ImplicitFunctionInfSiLU(ImplicitFunctionInf):\n",
    "    \"\"\"\n",
    "    An implicit function that uses the SiLU nonlinearity.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def phi(X):\n",
    "        return X * torch.sigmoid(X)\n",
    "\n",
    "    @staticmethod\n",
    "    def dphi(X):\n",
    "        grad = X.clone().detach()\n",
    "        sigmoid = torch.sigmoid(grad)\n",
    "        return sigmoid * (1 + grad * (1 - sigmoid))\n",
    "\n",
    "class ImplicitFunctionInfSIREN(ImplicitFunctionInf):\n",
    "    \"\"\"\n",
    "    An implicit function that uses the SIREN nonlinearity.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def phi(X):\n",
    "        return torch.sin(X)\n",
    "\n",
    "    @staticmethod\n",
    "    def dphi(X):\n",
    "        grad = X.clone().detach()\n",
    "        return grad*torch.cos(X)\n",
    "\n",
    "# ---------------------------\n",
    "# Implicit Models with Fourier Features\n",
    "# ---------------------------\n",
    "class ImplicitModelFourier(nn.Module):\n",
    "    def __init__(self, hidden_features=128, in_features=1, out_features=1, num_fourier_features=16, f=ImplicitFunctionInf):\n",
    "        super().__init__()\n",
    "        self.num_fourier_features = num_fourier_features\n",
    "        self.net = ImplicitModel(hidden_features, 2*self.num_fourier_features, out_features, f=f)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ffeat = create_fourier_features(x, num_features=self.num_fourier_features)\n",
    "        return self.net(ffeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 51076 parameters\n",
      "Epoch 500/1000, Loss: 0.554905\n",
      "Epoch 1000/1000, Loss: 0.554905\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Model & Training Setup\n",
    "# ---------------------------\n",
    "# model, lr = MLP(hidden_features=64, hidden_layers=13), 1e-4\n",
    "\n",
    "# model, lr = SIREN(hidden_features=128, hidden_layers=3), 1e-4\n",
    "\n",
    "# model, lr = FourierMLP(\n",
    "#     in_features=1,\n",
    "#     num_fourier_features=8,\n",
    "#     hidden_features=128,\n",
    "#     hidden_layers=4,\n",
    "#     out_features=1\n",
    "# ), 1e-3\n",
    "\n",
    "# model, lr = ImplicitModelFourier(220, 1, 1, 8, f=ImplicitFunctionInf), 1e-2; torch.nn.init.normal_(fuse_parameters(model), mean=0., std=0.1)\n",
    "\n",
    "model, lr = ImplicitModel(225, 1, 1, f=ImplicitFunctionInf), 1e-2; torch.nn.init.normal_(fuse_parameters(model), mean=0., std=0.1)\n",
    "\n",
    "\n",
    "print(f'model size: {sum(p.numel() for p in model.parameters())} parameters')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# scheduler = StepLR(optimizer, step_size=500, gamma=0.5)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.99)\n",
    "\n",
    "# Move data/model to GPU if available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "x_train = x_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "\n",
    "\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_pred = model(x_train)\n",
    "    # Use multi-scale loss\n",
    "    loss = multiscale_loss(y_pred, y_train, scales=[1, 2, 4, 8])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    if (epoch+1) % 500 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Evaluation\n",
    "# ---------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x_eval = torch.linspace(0.5, 3.5, 1000, device=device).unsqueeze(-1)\n",
    "    y_eval = weierstrass(x_eval.squeeze(-1))\n",
    "    y_pred = model(x_eval)\n",
    "\n",
    "x_eval_cpu = x_eval.cpu().numpy().flatten()\n",
    "y_eval_cpu = y_eval.cpu().numpy().flatten()\n",
    "y_pred_cpu = y_pred.cpu().numpy().flatten()\n",
    "\n",
    "# ---------------------------\n",
    "# Plot Results\n",
    "# ---------------------------\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(x_eval_cpu, y_eval_cpu, label='True Weierstrass Function', linewidth=2)\n",
    "plt.plot(x_eval_cpu, y_pred_cpu, label='NN Approximation', linewidth=2, linestyle='--')\n",
    "# plot a vertical line at x=2\n",
    "plt.axvline(x=2.5, color='gray', linestyle='--', linewidth=2)\n",
    "plt.legend(fontsize=14)\n",
    "plt.title(\"Approximation of Weierstrass Function\", fontsize=16)\n",
    "plt.xlabel(\"x\", fontsize=14)\n",
    "plt.ylabel(\"f(x)\", fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()\n",
    "plt.savefig(\"output_plot.png\")\n",
    "\n",
    "# save data\n",
    "import pickle\n",
    "with open('weier_imp_L.pkl', 'wb') as f:\n",
    "    pickle.dump({'x': x_eval_cpu, 'y_true': y_eval_cpu, 'y_pred': y_pred_cpu}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Plot all Results\n",
    "# ---------------------------\n",
    "with open('weier_fourier_S.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "x_eval_cpu = data['x']\n",
    "y_eval_cpu = data['y_true']\n",
    "y_pred_cpu = data['y_pred']\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(x_eval_cpu, y_eval_cpu, label='True Weierstrass Function', linewidth=2)\n",
    "plt.plot(x_eval_cpu, y_pred_cpu, label='NN Approximation', linewidth=2, linestyle='--')\n",
    "# plot a vertical line at x=2\n",
    "plt.axvline(x=2.5, color='gray', linestyle='--', linewidth=2)\n",
    "plt.legend(fontsize=14)\n",
    "plt.title(\"Approximation of Weierstrass Function\", fontsize=16)\n",
    "plt.xlabel(\"x\", fontsize=14)\n",
    "plt.ylabel(\"f(x)\", fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()\n",
    "plt.savefig(\"output_plot.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
